{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 18:14:26.012895: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.config import list_physical_devices\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.config.experimental import set_memory_growth\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "NUM_HEAD = 8\n",
    "FF_DIM = 1024\n",
    "NUM_LAYERS = 8\n",
    "EMBED_DIM = 256\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 18:14:27.331918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:14:27.349077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:14:27.349777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:14:27.350896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        set_memory_growth(gpu, True)\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import gdown\n",
    "from processing import load_dicts, count_samples, data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1iXflqpzg-9dIBTs4ZejCTfusgu-dUHpN test_paragraphs.txt\n",
      "Processing file 14s-iXY0PKEOIGtZnX-yweFtgz31owdFG train_paragraphs.txt\n",
      "Processing file 1xASDq01S0Zis8PBNN8ZAUtTD6ZEhso4V val_paragraphs.txt\n",
      "Building directory structure completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder list completed\n",
      "Building directory structure\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1iXflqpzg-9dIBTs4ZejCTfusgu-dUHpN\n",
      "To: /home/mkrtich/arm_mlm/char_paragraphs/test_paragraphs.txt\n",
      "100%|██████████| 99.5M/99.5M [00:02<00:00, 33.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14s-iXY0PKEOIGtZnX-yweFtgz31owdFG\n",
      "To: /home/mkrtich/arm_mlm/char_paragraphs/train_paragraphs.txt\n",
      "100%|██████████| 799M/799M [00:26<00:00, 30.0MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xASDq01S0Zis8PBNN8ZAUtTD6ZEhso4V\n",
      "To: /home/mkrtich/arm_mlm/char_paragraphs/val_paragraphs.txt\n",
      "100%|██████████| 99.5M/99.5M [00:02<00:00, 33.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/mkrtich/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "folder_url = 'https://drive.google.com/drive/folders/1rk9HhT6OtrGlqC2ZGT0eREFc8FH2F8QJ?usp=sharing'\n",
    "if 'data' not in listdir():\n",
    "    gdown.download_folder(folder_url)\n",
    "    ! mv char_paragraphs data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "files not existing. created new files.\n",
      "Warning: an older trained model might not work with new character indexing\n"
     ]
    }
   ],
   "source": [
    "char2idx, idx2char = load_dicts()\n",
    "MASK_ID = len(char2idx)\n",
    "VOCAB_SIZE = MASK_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train, Val, Test\n",
      " 1159732 144965 144969\n"
     ]
    }
   ],
   "source": [
    "N_TRAIN = count_samples('data/train_paragraphs.txt')\n",
    "N_STEPS = int(N_TRAIN/BATCH_SIZE)+1\n",
    "N_VAL = count_samples('data/val_paragraphs.txt')\n",
    "N_STEPS_VAL= int(N_VAL/BATCH_SIZE)+1\n",
    "N_TEST = count_samples('data/test_paragraphs.txt')\n",
    "N_STEPS_TEST= int(N_TEST/BATCH_SIZE)+1\n",
    "print(' Train, Val, Test\\n', N_TRAIN, N_VAL, N_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_generator('data/train_paragraphs.txt', max_len=512, masking=True, batch_size=256, vocab=char2idx, mask_id=None)\n",
    "val_gen = data_generator('data/val_paragraphs.txt', max_len=512, masking=True, batch_size=256, vocab=char2idx, mask_id=None)\n",
    "test_gen = data_generator('data/test_paragraphs.txt', max_len=512, masking=True, batch_size=256, vocab=char2idx, mask_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 18:15:15.236781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-18 18:15:15.238357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:15:15.239119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:15:15.239826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:15:15.616602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:15:15.617292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:15:15.617924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 18:15:15.618273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22129 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from modeling import ModelConfigurator, MaskedLanguageModel, create_mlm, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelConfigurator(\n",
    "    MAX_LEN,NUM_HEAD,\n",
    "    FF_DIM,\n",
    "    NUM_LAYERS,\n",
    "    EMBED_DIM,\n",
    "    LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 512, 256)     47360       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 512, 256)    0           ['word_embedding[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 512, 256)    263168      ['tf.__operators__.add[0][0]',   \n",
      " MultiHeadAttention)                                              'tf.__operators__.add[0][0]',   \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 512, 256)    0           ['tf.__operators__.add[0][0]',   \n",
      " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_1[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 512, 256)    0           ['encoder_0/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_2[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_1/multiheadattention (  (None, 512, 256)    263168      ['encoder_0/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_1/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_1/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 512, 256)    0           ['encoder_0/ffn_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_1/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_1/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_3[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_1/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_1/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_1/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_1/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 512, 256)    0           ['encoder_1/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_1/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_1/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_4[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_2/multiheadattention (  (None, 512, 256)    263168      ['encoder_1/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_1/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_1/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_2/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_2/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 512, 256)    0           ['encoder_1/ffn_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_2/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_2/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_5[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_2/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_2/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_2/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_2/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 512, 256)    0           ['encoder_2/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_2/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_2/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_6[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_3/multiheadattention (  (None, 512, 256)    263168      ['encoder_2/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_2/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_2/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_3/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_3/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 512, 256)    0           ['encoder_2/ffn_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_3/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_3/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_7[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_3/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_3/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_3/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_3/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 512, 256)    0           ['encoder_3/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_3/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_3/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_8[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_4/multiheadattention (  (None, 512, 256)    263168      ['encoder_3/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_3/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_3/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_4/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_4/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 512, 256)    0           ['encoder_3/ffn_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_4/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_4/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_9[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_4/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_4/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_4/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_4/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 512, 256)    0           ['encoder_4/att_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_4/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_4/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_10[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_5/multiheadattention (  (None, 512, 256)    263168      ['encoder_4/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_4/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_4/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_5/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_5/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 512, 256)    0           ['encoder_4/ffn_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_5/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_5/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_11[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_5/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_5/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_5/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_5/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 512, 256)    0           ['encoder_5/att_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_5/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_5/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_12[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_6/multiheadattention (  (None, 512, 256)    263168      ['encoder_5/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_5/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_5/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_6/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_6/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 512, 256)    0           ['encoder_5/ffn_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_6/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_6/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_13[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_6/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_6/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_6/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_6/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 512, 256)    0           ['encoder_6/att_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_6/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_6/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_14[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_7/multiheadattention (  (None, 512, 256)    263168      ['encoder_6/ffn_layernormalizatio\n",
      " MultiHeadAttention)                                             n[0][0]',                        \n",
      "                                                                  'encoder_6/ffn_layernormalizatio\n",
      "                                                                 n[0][0]',                        \n",
      "                                                                  'encoder_6/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_7/att_dropout (Dropout  (None, 512, 256)    0           ['encoder_7/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 512, 256)    0           ['encoder_6/ffn_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_7/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_7/att_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_15[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_7/ffn (Sequential)     (None, 512, 256)     525568      ['encoder_7/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_7/ffn_dropout (Dropout  (None, 512, 256)    0           ['encoder_7/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 512, 256)    0           ['encoder_7/att_layernormalizatio\n",
      " ambda)                                                          n[0][0]',                        \n",
      "                                                                  'encoder_7/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_7/ffn_layernormalizati  (None, 512, 256)    512         ['tf.__operators__.add_16[0][0]']\n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 512, 185)     47545       ['encoder_7/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,412,985\n",
      "Trainable params: 6,412,985\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "masked_model = create_mlm(mc, VOCAB_SIZE)\n",
    "masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 2.0213 - accuracy: 0.2678\n",
      "Epoch 1: val_loss improved from inf to 0.90695, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3303s 728ms/step - loss: 2.0213 - accuracy: 0.2679 - val_loss: 0.9069 - val_accuracy: 0.7399 - lr: 4.0000e-04\n",
      "Epoch 2/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.7491\n",
      "Epoch 2: val_loss improved from 0.90695 to 0.56812, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3305s 729ms/step - loss: 0.7824 - accuracy: 0.7491 - val_loss: 0.5681 - val_accuracy: 0.8363 - lr: 4.0000e-04\n",
      "Epoch 3/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.5684 - accuracy: 0.8264\n",
      "Epoch 3: val_loss improved from 0.56812 to 0.45074, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3296s 727ms/step - loss: 0.5684 - accuracy: 0.8264 - val_loss: 0.4507 - val_accuracy: 0.8703 - lr: 4.0000e-04\n",
      "Epoch 4/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.4799 - accuracy: 0.8562\n",
      "Epoch 4: val_loss improved from 0.45074 to 0.39126, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3299s 728ms/step - loss: 0.4799 - accuracy: 0.8562 - val_loss: 0.3913 - val_accuracy: 0.8861 - lr: 4.0000e-04\n",
      "Epoch 5/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8717\n",
      "Epoch 5: val_loss improved from 0.39126 to 0.35758, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3285s 725ms/step - loss: 0.4318 - accuracy: 0.8717 - val_loss: 0.3576 - val_accuracy: 0.8956 - lr: 4.0000e-04\n",
      "Epoch 6/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8814\n",
      "Epoch 6: val_loss improved from 0.35758 to 0.33567, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3285s 725ms/step - loss: 0.4003 - accuracy: 0.8814 - val_loss: 0.3357 - val_accuracy: 0.9024 - lr: 4.0000e-04\n",
      "Epoch 7/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8882\n",
      "Epoch 7: val_loss improved from 0.33567 to 0.31618, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3285s 725ms/step - loss: 0.3779 - accuracy: 0.8882 - val_loss: 0.3162 - val_accuracy: 0.9074 - lr: 4.0000e-04\n",
      "Epoch 8/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.8935\n",
      "Epoch 8: val_loss improved from 0.31618 to 0.30695, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3285s 725ms/step - loss: 0.3604 - accuracy: 0.8935 - val_loss: 0.3069 - val_accuracy: 0.9110 - lr: 4.0000e-04\n",
      "Epoch 9/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.8977\n",
      "Epoch 9: val_loss improved from 0.30695 to 0.29306, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3284s 725ms/step - loss: 0.3467 - accuracy: 0.8977 - val_loss: 0.2931 - val_accuracy: 0.9148 - lr: 4.0000e-04\n",
      "Epoch 10/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9009\n",
      "Epoch 10: val_loss improved from 0.29306 to 0.28395, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3283s 725ms/step - loss: 0.3357 - accuracy: 0.9009 - val_loss: 0.2839 - val_accuracy: 0.9169 - lr: 4.0000e-04\n",
      "Epoch 11/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.9037\n",
      "Epoch 11: val_loss improved from 0.28395 to 0.27746, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3239s 715ms/step - loss: 0.3266 - accuracy: 0.9037 - val_loss: 0.2775 - val_accuracy: 0.9192 - lr: 4.0000e-04\n",
      "Epoch 12/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.9060\n",
      "Epoch 12: val_loss improved from 0.27746 to 0.27082, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3244s 716ms/step - loss: 0.3187 - accuracy: 0.9060 - val_loss: 0.2708 - val_accuracy: 0.9208 - lr: 4.0000e-04\n",
      "Epoch 13/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.9083\n",
      "Epoch 13: val_loss improved from 0.27082 to 0.26237, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3236s 714ms/step - loss: 0.3113 - accuracy: 0.9083 - val_loss: 0.2624 - val_accuracy: 0.9232 - lr: 4.0000e-04\n",
      "Epoch 14/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9100\n",
      "Epoch 14: val_loss improved from 0.26237 to 0.25931, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3238s 715ms/step - loss: 0.3054 - accuracy: 0.9100 - val_loss: 0.2593 - val_accuracy: 0.9245 - lr: 4.0000e-04\n",
      "Epoch 15/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.9118\n",
      "Epoch 15: val_loss improved from 0.25931 to 0.25697, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3244s 716ms/step - loss: 0.2997 - accuracy: 0.9118 - val_loss: 0.2570 - val_accuracy: 0.9253 - lr: 4.0000e-04\n",
      "Epoch 16/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.9130\n",
      "Epoch 16: val_loss improved from 0.25697 to 0.24945, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3217s 710ms/step - loss: 0.2953 - accuracy: 0.9130 - val_loss: 0.2494 - val_accuracy: 0.9271 - lr: 4.0000e-04\n",
      "Epoch 17/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9145\n",
      "Epoch 17: val_loss improved from 0.24945 to 0.24684, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3217s 710ms/step - loss: 0.2906 - accuracy: 0.9145 - val_loss: 0.2468 - val_accuracy: 0.9280 - lr: 4.0000e-04\n",
      "Epoch 18/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9158\n",
      "Epoch 18: val_loss improved from 0.24684 to 0.24452, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3216s 710ms/step - loss: 0.2861 - accuracy: 0.9158 - val_loss: 0.2445 - val_accuracy: 0.9290 - lr: 4.0000e-04\n",
      "Epoch 19/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9169\n",
      "Epoch 19: val_loss improved from 0.24452 to 0.24235, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3218s 710ms/step - loss: 0.2822 - accuracy: 0.9169 - val_loss: 0.2423 - val_accuracy: 0.9298 - lr: 4.0000e-04\n",
      "Epoch 20/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9178\n",
      "Epoch 20: val_loss improved from 0.24235 to 0.23829, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3217s 710ms/step - loss: 0.2792 - accuracy: 0.9178 - val_loss: 0.2383 - val_accuracy: 0.9308 - lr: 4.0000e-04\n",
      "Epoch 21/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.9188\n",
      "Epoch 21: val_loss improved from 0.23829 to 0.23516, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3216s 710ms/step - loss: 0.2759 - accuracy: 0.9188 - val_loss: 0.2352 - val_accuracy: 0.9316 - lr: 4.0000e-04\n",
      "Epoch 22/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9197\n",
      "Epoch 22: val_loss improved from 0.23516 to 0.23356, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3217s 710ms/step - loss: 0.2731 - accuracy: 0.9197 - val_loss: 0.2336 - val_accuracy: 0.9320 - lr: 4.0000e-04\n",
      "Epoch 23/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9251\n",
      "Epoch 23: val_loss improved from 0.23356 to 0.21267, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3218s 710ms/step - loss: 0.2498 - accuracy: 0.9251 - val_loss: 0.2127 - val_accuracy: 0.9378 - lr: 6.6667e-05\n",
      "Epoch 24/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9281\n",
      "Epoch 24: val_loss improved from 0.21267 to 0.20939, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3216s 710ms/step - loss: 0.2419 - accuracy: 0.9281 - val_loss: 0.2094 - val_accuracy: 0.9387 - lr: 6.6667e-05\n",
      "Epoch 25/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9292\n",
      "Epoch 25: val_loss improved from 0.20939 to 0.20793, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3233s 714ms/step - loss: 0.2389 - accuracy: 0.9292 - val_loss: 0.2079 - val_accuracy: 0.9391 - lr: 6.6667e-05\n",
      "Epoch 26/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9299\n",
      "Epoch 26: val_loss improved from 0.20793 to 0.20536, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3242s 716ms/step - loss: 0.2366 - accuracy: 0.9299 - val_loss: 0.2054 - val_accuracy: 0.9398 - lr: 6.6667e-05\n",
      "Epoch 27/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9303\n",
      "Epoch 27: val_loss improved from 0.20536 to 0.20348, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3240s 715ms/step - loss: 0.2351 - accuracy: 0.9303 - val_loss: 0.2035 - val_accuracy: 0.9403 - lr: 6.6667e-05\n",
      "Epoch 28/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9307\n",
      "Epoch 28: val_loss did not improve from 0.20348\n",
      "4531/4531 [==============================] - 3212s 709ms/step - loss: 0.2337 - accuracy: 0.9307 - val_loss: 0.2037 - val_accuracy: 0.9404 - lr: 6.6667e-05\n",
      "Epoch 29/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.9312\n",
      "Epoch 29: val_loss improved from 0.20348 to 0.20061, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3217s 710ms/step - loss: 0.2322 - accuracy: 0.9312 - val_loss: 0.2006 - val_accuracy: 0.9410 - lr: 6.6667e-05\n",
      "Epoch 30/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9315\n",
      "Epoch 30: val_loss improved from 0.20061 to 0.20017, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3218s 710ms/step - loss: 0.2312 - accuracy: 0.9315 - val_loss: 0.2002 - val_accuracy: 0.9414 - lr: 6.6667e-05\n",
      "Epoch 31/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9318\n",
      "Epoch 31: val_loss improved from 0.20017 to 0.19923, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3242s 716ms/step - loss: 0.2303 - accuracy: 0.9318 - val_loss: 0.1992 - val_accuracy: 0.9415 - lr: 6.6667e-05\n",
      "Epoch 32/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.9320\n",
      "Epoch 32: val_loss improved from 0.19923 to 0.19911, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3237s 714ms/step - loss: 0.2294 - accuracy: 0.9320 - val_loss: 0.1991 - val_accuracy: 0.9416 - lr: 6.6667e-05\n",
      "Epoch 33/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9322\n",
      "Epoch 33: val_loss improved from 0.19911 to 0.19722, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3228s 712ms/step - loss: 0.2284 - accuracy: 0.9322 - val_loss: 0.1972 - val_accuracy: 0.9421 - lr: 6.6667e-05\n",
      "Epoch 34/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.9326\n",
      "Epoch 34: val_loss improved from 0.19722 to 0.19688, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3223s 711ms/step - loss: 0.2273 - accuracy: 0.9326 - val_loss: 0.1969 - val_accuracy: 0.9422 - lr: 6.6667e-05\n",
      "Epoch 35/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9327\n",
      "Epoch 35: val_loss improved from 0.19688 to 0.19672, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3198s 706ms/step - loss: 0.2269 - accuracy: 0.9327 - val_loss: 0.1967 - val_accuracy: 0.9423 - lr: 6.6667e-05\n",
      "Epoch 36/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9328\n",
      "Epoch 36: val_loss improved from 0.19672 to 0.19489, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3199s 706ms/step - loss: 0.2263 - accuracy: 0.9328 - val_loss: 0.1949 - val_accuracy: 0.9426 - lr: 6.6667e-05\n",
      "Epoch 37/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9331\n",
      "Epoch 37: val_loss did not improve from 0.19489\n",
      "4531/4531 [==============================] - 3209s 708ms/step - loss: 0.2254 - accuracy: 0.9331 - val_loss: 0.1964 - val_accuracy: 0.9425 - lr: 6.6667e-05\n",
      "Epoch 38/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9331\n",
      "Epoch 38: val_loss improved from 0.19489 to 0.19395, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3218s 710ms/step - loss: 0.2250 - accuracy: 0.9331 - val_loss: 0.1940 - val_accuracy: 0.9430 - lr: 6.6667e-05\n",
      "Epoch 39/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9334\n",
      "Epoch 39: val_loss did not improve from 0.19395\n",
      "4531/4531 [==============================] - 3214s 709ms/step - loss: 0.2244 - accuracy: 0.9334 - val_loss: 0.1945 - val_accuracy: 0.9430 - lr: 6.6667e-05\n",
      "Epoch 40/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9336\n",
      "Epoch 40: val_loss improved from 0.19395 to 0.19181, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3220s 711ms/step - loss: 0.2239 - accuracy: 0.9336 - val_loss: 0.1918 - val_accuracy: 0.9435 - lr: 6.6667e-05\n",
      "Epoch 41/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9338\n",
      "Epoch 41: val_loss did not improve from 0.19181\n",
      "4531/4531 [==============================] - 3218s 710ms/step - loss: 0.2232 - accuracy: 0.9338 - val_loss: 0.1934 - val_accuracy: 0.9432 - lr: 6.6667e-05\n",
      "Epoch 42/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9339\n",
      "Epoch 42: val_loss did not improve from 0.19181\n",
      "4531/4531 [==============================] - 3224s 712ms/step - loss: 0.2226 - accuracy: 0.9339 - val_loss: 0.1928 - val_accuracy: 0.9433 - lr: 6.6667e-05\n",
      "Epoch 43/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9341\n",
      "Epoch 43: val_loss did not improve from 0.19181\n",
      "4531/4531 [==============================] - 3226s 712ms/step - loss: 0.2219 - accuracy: 0.9341 - val_loss: 0.1921 - val_accuracy: 0.9435 - lr: 6.6667e-05\n",
      "Epoch 44/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9342\n",
      "Epoch 44: val_loss did not improve from 0.19181\n",
      "4531/4531 [==============================] - 3228s 712ms/step - loss: 0.2218 - accuracy: 0.9342 - val_loss: 0.1919 - val_accuracy: 0.9437 - lr: 6.6667e-05\n",
      "Epoch 45/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9344\n",
      "Epoch 45: val_loss improved from 0.19181 to 0.19129, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3234s 714ms/step - loss: 0.2211 - accuracy: 0.9344 - val_loss: 0.1913 - val_accuracy: 0.9438 - lr: 6.6667e-05\n",
      "Epoch 46/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9345\n",
      "Epoch 46: val_loss improved from 0.19129 to 0.18994, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3196s 705ms/step - loss: 0.2206 - accuracy: 0.9345 - val_loss: 0.1899 - val_accuracy: 0.9441 - lr: 6.6667e-05\n",
      "Epoch 47/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9347\n",
      "Epoch 47: val_loss did not improve from 0.18994\n",
      "4531/4531 [==============================] - 3191s 704ms/step - loss: 0.2202 - accuracy: 0.9347 - val_loss: 0.1900 - val_accuracy: 0.9442 - lr: 6.6667e-05\n",
      "Epoch 48/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9348\n",
      "Epoch 48: val_loss did not improve from 0.18994\n",
      "4531/4531 [==============================] - 3199s 706ms/step - loss: 0.2197 - accuracy: 0.9348 - val_loss: 0.1902 - val_accuracy: 0.9442 - lr: 6.6667e-05\n",
      "Epoch 49/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9349\n",
      "Epoch 49: val_loss improved from 0.18994 to 0.18903, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3220s 711ms/step - loss: 0.2193 - accuracy: 0.9349 - val_loss: 0.1890 - val_accuracy: 0.9445 - lr: 6.6667e-05\n",
      "Epoch 50/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9350\n",
      "Epoch 50: val_loss did not improve from 0.18903\n",
      "4531/4531 [==============================] - 3218s 710ms/step - loss: 0.2189 - accuracy: 0.9350 - val_loss: 0.1892 - val_accuracy: 0.9444 - lr: 6.6667e-05\n",
      "Epoch 51/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9351\n",
      "Epoch 51: val_loss improved from 0.18903 to 0.18836, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3233s 713ms/step - loss: 0.2184 - accuracy: 0.9351 - val_loss: 0.1884 - val_accuracy: 0.9446 - lr: 6.6667e-05\n",
      "Epoch 52/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9352\n",
      "Epoch 52: val_loss improved from 0.18836 to 0.18675, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3200s 706ms/step - loss: 0.2182 - accuracy: 0.9352 - val_loss: 0.1868 - val_accuracy: 0.9450 - lr: 6.6667e-05\n",
      "Epoch 53/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9359\n",
      "Epoch 53: val_loss improved from 0.18675 to 0.18536, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3196s 705ms/step - loss: 0.2152 - accuracy: 0.9359 - val_loss: 0.1854 - val_accuracy: 0.9454 - lr: 1.1111e-05\n",
      "Epoch 54/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9363\n",
      "Epoch 54: val_loss improved from 0.18536 to 0.18480, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3222s 711ms/step - loss: 0.2141 - accuracy: 0.9363 - val_loss: 0.1848 - val_accuracy: 0.9457 - lr: 1.1111e-05\n",
      "Epoch 55/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9367\n",
      "Epoch 55: val_loss did not improve from 0.18480\n",
      "4531/4531 [==============================] - 3223s 711ms/step - loss: 0.2132 - accuracy: 0.9367 - val_loss: 0.1852 - val_accuracy: 0.9455 - lr: 1.1111e-05\n",
      "Epoch 56/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9368\n",
      "Epoch 56: val_loss improved from 0.18480 to 0.18471, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3246s 716ms/step - loss: 0.2128 - accuracy: 0.9368 - val_loss: 0.1847 - val_accuracy: 0.9455 - lr: 1.1111e-05\n",
      "Epoch 57/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9369\n",
      "Epoch 57: val_loss improved from 0.18471 to 0.18342, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3237s 714ms/step - loss: 0.2125 - accuracy: 0.9369 - val_loss: 0.1834 - val_accuracy: 0.9460 - lr: 1.1111e-05\n",
      "Epoch 58/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9368\n",
      "Epoch 58: val_loss did not improve from 0.18342\n",
      "4531/4531 [==============================] - 3220s 711ms/step - loss: 0.2125 - accuracy: 0.9368 - val_loss: 0.1846 - val_accuracy: 0.9457 - lr: 1.1111e-05\n",
      "Epoch 59/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9369\n",
      "Epoch 59: val_loss did not improve from 0.18342\n",
      "4531/4531 [==============================] - 3229s 713ms/step - loss: 0.2123 - accuracy: 0.9369 - val_loss: 0.1840 - val_accuracy: 0.9458 - lr: 1.1111e-05\n",
      "Epoch 60/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9369\n",
      "Epoch 60: val_loss did not improve from 0.18342\n",
      "4531/4531 [==============================] - 3233s 713ms/step - loss: 0.2123 - accuracy: 0.9369 - val_loss: 0.1834 - val_accuracy: 0.9461 - lr: 1.1111e-05\n",
      "Epoch 61/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9370\n",
      "Epoch 61: val_loss did not improve from 0.18342\n",
      "4531/4531 [==============================] - 3234s 714ms/step - loss: 0.2119 - accuracy: 0.9370 - val_loss: 0.1840 - val_accuracy: 0.9458 - lr: 1.1111e-05\n",
      "Epoch 62/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9370\n",
      "Epoch 62: val_loss improved from 0.18342 to 0.18310, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3243s 716ms/step - loss: 0.2118 - accuracy: 0.9370 - val_loss: 0.1831 - val_accuracy: 0.9461 - lr: 1.1111e-05\n",
      "Epoch 63/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9371\n",
      "Epoch 63: val_loss did not improve from 0.18310\n",
      "4531/4531 [==============================] - 3234s 714ms/step - loss: 0.2114 - accuracy: 0.9371 - val_loss: 0.1832 - val_accuracy: 0.9461 - lr: 1.1111e-05\n",
      "Epoch 64/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9371\n",
      "Epoch 64: val_loss did not improve from 0.18310\n",
      "4531/4531 [==============================] - 3242s 716ms/step - loss: 0.2114 - accuracy: 0.9371 - val_loss: 0.1832 - val_accuracy: 0.9460 - lr: 1.1111e-05\n",
      "Epoch 65/65\n",
      "4531/4531 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9372\n",
      "Epoch 65: val_loss improved from 0.18310 to 0.18299, saving model to models/TF_char_6412k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/TF_char_6412k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531/4531 [==============================] - 3249s 717ms/step - loss: 0.2114 - accuracy: 0.9372 - val_loss: 0.1830 - val_accuracy: 0.9462 - lr: 1.1111e-05\n"
     ]
    }
   ],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 22:\n",
    "        lr = LR\n",
    "    elif epoch < 52:\n",
    "        lr = LR/6\n",
    "    else:\n",
    "        lr = LR/36\n",
    "    return lr\n",
    "lr_callback = callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath='models/TF_char_6412k',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    "    )\n",
    "\n",
    "hist = masked_model.fit(\n",
    "    train_gen, epochs=65,\n",
    "    steps_per_epoch=N_STEPS,\n",
    "    validation_steps=N_STEPS_VAL,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=[lr_callback, checkpoint]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567/567 [==============================] - 139s 246ms/step - loss: 0.1835 - accuracy: 0.9460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1835266500711441, 0.9459105730056763]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_model.evaluate(test_gen, steps=N_STEPS_TEST, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11efdfc52e0f02ce3ff7bcd7d668219934d7618071bbd75fc98a62b6d20d0895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
